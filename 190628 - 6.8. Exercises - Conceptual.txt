### Chapter 6, Exercise 1

(a) best subset
(b) forward stepwise?
(c)
i. True
ii. True
iii. True
iv. True
v. True

Answers:

a

Best subset selection has the smallest training RSS because the other two methods determine models with a path dependency on which predictors they pick first as they iterate to the k'th model.

b (*)

Best subset selection may have the smallest test RSS because it considers more models then the other methods. However, the other models might have better luck picking a model that fits the test data better.

c

i. True. ii. True. iii. False. iv. False. v. False.


### Chapter 6, Exercise 2

(a) ii.
(b) ii.
(c) ii.

Answers:

a (Lasso)

iii. Less flexible and better predictions because of less variance, more bias

b (Ridge regression)

Same as lasso. iii.


c (Non-linear methods)

ii. More flexible, less bias, more variance 


### Chapter 6, Exercise 3

(a) training RSS: v
(b) test RSS: v
(c) variance: iV
(d) bias: iii
(e) irreducble error: v

Answers:


Chapter 6: Exercise 3
a

(iv) Steadily decreases: As we increase s
from 0, all β 's increase from 0 to their least square estimate values. Training error for 0 β

s is the maximum and it steadily decreases to the Ordinary Least Square RSS
b

(ii) Decrease initially, and then eventually start increasing in a U shape: When s=0
, all β s are 0, the model is extremely simple and has a high test RSS. As we increase s, beta s assume non-zero values and model starts fitting well on test data and so test RSS decreases. Eventually, as beta

s approach their full blown OLS values, they start overfitting to the training data, increasing test RSS.
c

(iii) Steadily increase: When s=0
, the model effectively predicts a constant and has almost no variance. As we increase s, the models includes more β s and their values start increasing. At this point, the values of β

s become highly dependent on training data, thus increasing the variance.
d

(iv) Steadily decrease: When s=0
, the model effectively predicts a constant and hence the prediction is far from actual value. Thus bias is high. As s increases, more β

s become non-zero and thus the model continues to fit training data better. And thus, bias decreases.
e

(v) Remains constant: By definition, irreducible error is model independent and hence irrespective of the choice of s

, remains constant.




### Chapter 6, Exercise 4

(a) training RSS: iii
(b) test RSS: ii
(c) variance: iv
(d) bias: iii
(e) irreducble error: v


Chapter 6: Exercise 4
a
(iii) Steadily increase: As we increase λ from 0, all β 's decrease from their least square estimate values to 0. Training error for full-blown-OLS β s is the minimum and it steadily increases as β s are reduced to 0.

b
(ii) Decrease initially, and then eventually start increasing in a U shape: When λ=0, all β s have their least square estimate values. In this case, the model tries to fit hard to training data and hence test RSS is high. As we increase λ, beta s start reducing to zero and some of the overfitting is reduced. Thus, test RSS initially decreases. Eventually, as beta s approach 0, the model becomes too simple and test RSS increases.

c
(iv) Steadily decreases: When λ=0, the β s have their least square estimate values. The actual estimates heavily depend on the training data and hence variance is high. As we increase λ, β s start decreasing and model becomes simpler. In the limiting case of λ approaching infinity, all beta s reduce to zero and model predicts a constant and has no variance.

d
(iii) Steadily increases: When λ=0, β s have their least-square estimate values and hence have the least bias. As λ increases, β s start reducing towards zero, the model fits less accurately to training data and hence bias increases. In the limiting case of λ approaching infinity, the model predicts a constant and hence bias is maximum.

e
(v) Remains constant: By definition, irreducible error is model independent and hence irrespective of the choice of λ, remains constant.